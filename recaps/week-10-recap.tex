\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pifont}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{float,graphicx}
\usepackage{framed}
\usepackage[margin=3cm, headheight=15pt]{geometry}

\pagestyle{fancyplain}
\fancypagestyle{plain}{
	\renewcommand{\headrulewidth}{0.4pt}
}

\lhead{\fancyplain{Pranav Rao}{Pranav Rao}}
\rhead{\fancyplain{Week 10 Recap}{Week 10 Recap}}

\title{Week 10 Recap}
\author{Pranav Rao}
\date{Sunday, November 26, 2023}

\begin{document}
\maketitle

\section{Methods of Finding PDF of Functions of Random Variables}

\subsection{CDF Method of Finding PDF of a Function of a Random Variable}

Let $U$ be a function of the random variables $Y$.
\begin{enumerate}
	\item Find the region $U \leq u$.
	\item Find $F_U(u) = P(U \leq u)$ by integrating $f(y)$ over the region of $U \leq u$ .
	\item Find the density function $f_U(u)$ by differentiating $F_U(u)$. Thus,
	      $f_U(u) = \frac{dF_U(u)}{du}$
\end{enumerate}

\subsection{Method of Transformations to Find PDF of Function of a Random Variable}

\begin{itemize}
	\item If $g(X)$ is monotonic (i.e. either strictly increasing or decreasing over the range of $X$) so it is invertible with inverse function $X = g^{-1}(Y)$, then:
	      \[
		      f_Y(y) = f_X(g^{-1}(y)) \left|\frac{d}{dy} g^{-1}(y)\right|
	      \]
\end{itemize}

\section{Using MGFs to Determine Distribution of Sum of IID Random Variables}
\begin{itemize}
	\item \textbf{IID Random Variables:} independent and independently
	      distributed (have the same distribution)
	\item Let $X$ and $Y$ be independent variables with moment generating
	      functions $M_X(t)$, $M_Y(t)$, respectively. Then the MGF of $X + Y$
	      can be found as follows:
	      \begin{align*}
		      M_{X + Y}(t) = E[e^{tX+tY}] = E[e^{tX}]E[e^{tY}] = M_X(t)M_Y(t)
	      \end{align*}
	\item If you calculate the above function and compare it with the
	      MGFs of known functions, you can find the distribution of the sum
	      $X + Y$.
\end{itemize}

\section{Bivariate Transformations Using Jacobians}

Suppose that $Y_1$, $Y_2$ are continuous random variables with joint density
function $f_{Y_1,Y_2}(y_1, y_2)$ and that for all $(y_1, y_2)$ such that
$f_{Y_1, Y_2}(y_1, y_2) > 0$.

\[
	u_1 = h_1(y_1, y_2) \text{ and } u_2 = h_2(y_1, y_2)
\]

is a one-to-one transformation from $(y_1, y_2)$ to $(u_1, u_2)$ with inverse:

\[
	y_1 = h^{-1}(u_1, u_2) \text{ and } y_2 = h_2^{-1}(u_1, u_2)
\]

If $h^{-1}(u_1, u_2)$ and $h_2^{-1}(u_1, u_2)$ have continuous partial derivatives with respect to $u_1, u_2$ and the \textbf{Jacobian}:

\[
	J = det \begin{bmatrix}
		\frac{\partial h_{1}^{-1}}{\partial u_1} & \frac{\partial h_{1}^{-1}}{\partial u_2} \\
		\frac{\partial h_{2}^{-1}}{\partial u_1} & \frac{\partial h_{2}^{-1}}{\partial u_2}
	\end{bmatrix} = \frac{\partial h_1^{-1}}{\partial u_1} \frac{\partial h_2^{-1}}{\partial u_2} - \frac{\partial h_2^{-1}}{\partial u_1} \frac{\partial h_1^{-1}}{\partial u_2} \neq 0
\]

Then, the joint density function of $U_1$ and $U_2$ is:

\[
	f_{U_1, U_2}(u_1, u_2) = f_{Y_1, Y_2}(h_1^{-1}(u_1, u_2), h_2^{-1}(u_1, u_2)) \left|J\right|
\]

where $\left|J\right|$ is the absolute value of the Jacobian.
\end{document}
